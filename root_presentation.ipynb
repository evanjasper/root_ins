{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Root Ad Auction Dataset\n",
    "## Team: The Puffy Shirts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the data set:\n",
    "\n",
    "    30 days of CSV files    \n",
    "    rows are winning bids in a second-price auction\n",
    "    contains information on the target user/platform, the UTC timestamp, the app, the exchange, etc...\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges of this data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large data set:\n",
    "\n",
    "- 1.42 GB as .zip / 8.19 GB as .csv\n",
    "    \n",
    "- 36.3M rows, 26 columns\n",
    "    \n",
    "Highly unbalanced categorical classification problem:\n",
    "\n",
    "- 1.57M clicks (4.3% of rows)\n",
    "    \n",
    "- 2,109 installs (0.0058% of rows, 0.13% of clicks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.dpi'] = 240 # fix high-dpi display scaling issues\n",
    "\n",
    "sys.path.append(os.getcwd()) # add cwd to path\n",
    "\n",
    "from zip_codes import ZC # zip code database\n",
    "import load_file as lf # file i/o\n",
    "import myplots as mp # my plotting functions\n",
    "zc = ZC('') # initialize zip code class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_27 = '2019-04-27.csv'\n",
    "data_dir = r'C:\\PythonBC\\RootData'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "df_27 = pd.read_csv( os.path.join(data_dir, fname_27))\n",
    "end = time.time()\n",
    "print(f'loading {fname_27} with default pd.read_csv() settings takes'\n",
    "      + f' {end-start:3.2f} seconds and {lf.mem_usage(df_27)} of RAM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some columns can be immediately dropped:\n",
    "- auction_id (meaningless string)\n",
    "- platform_os (all 'Android' or '-1')\n",
    "- app_bundle (all the same)\n",
    "- year (UTC time)\n",
    "- month (UTC time)\n",
    "- day (UTC time)\n",
    "- day_of_week (UTC time, incorrectly calculated)\n",
    "- hour (UTC time)\n",
    "- creative_size (redundant with creative_type)\n",
    "\n",
    "most columns are categorical with only a few unique values:\n",
    "- inventory_source (only 4 possible values)\n",
    "- category\n",
    "- platform_bandwidth\n",
    "- platform_carrier\n",
    "- platform_device_make (only 5 possible values)\n",
    "- platform_device_model\n",
    "- platform_device_screen_size\n",
    "- geo_zip\n",
    "\n",
    "some are boolean:\n",
    "- inventory_interstitial\n",
    "- rewarded\n",
    "- clicks\n",
    "- installs\n",
    "\n",
    "some columns need additional work:\n",
    "- geo_zip ('43212.0' >>> '43212')\n",
    "- category & segments are unsorted string lists\n",
    "\n",
    "some columns have multiple nan values:\n",
    "- platform_carrier\n",
    "- platform_device_make\n",
    "- platform_device_model\n",
    "- platform_device_screen_size\n",
    "\n",
    "The function lf.load_data() drops the useless columns and cleans & downcasts the others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "#df_27 = lf.load_wrapper(fname=fname_27, data_dir=data_dir)\n",
    "df_27 = lf.load_data(fname=fname_27, data_dir=data_dir)\n",
    "end = time.time()\n",
    "print(f'loading {fname_27} with downcasting takes {end-start:3.2f}'\n",
    "      + f' seconds and {lf.mem_usage(df_27)} of RAM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these tricks we reduce the RAM usage by a factor of **9.1**, but increase the load time by 43%. With this RAM reduction we can comfortably load the entire dataset into memory (~ 850 MB).\n",
    "\n",
    "We can quickly save the processed dataframe to disk using pd.to_parquet():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_save = time.time()\n",
    "lf.temp_save(df_27, fname=os.path.join(data_dir, fname_27.split('.')[0]+'.gzip'))\n",
    "end_save = time.time()\n",
    "df_27 = lf.temp_load( os.path.join(data_dir, fname_27.split('.')[0]+'.gzip') )\n",
    "end_load = time.time()\n",
    "\n",
    "print(f'saving to .gzip takes {end_save-start_save:3.2f}'\n",
    "      + f' seconds and loading from .gzip takes {end_load-end_save:3.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because parquet is so fast, it makes sense to pre-process all the data and save it to disk for later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the geo_zip column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a zip code database (https://www.unitedstateszipcodes.org/zip-code-database/) to get local information for each bid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myzip = ['43210']\n",
    "print(f'zip code {myzip[0]}:')\n",
    "print( f'   state: {zc.zip_to_state_2(myzip)[0]}' )\n",
    "print( f'   county: {zc.zip_to_county_2(myzip)[0]}' )\n",
    "print( f'   timezone: {zc.zip_to_tz_2(myzip)[0]}' )\n",
    "print( f'   coordinates: {zc.zip_to_lat_2(myzip)[0], zc.zip_to_lon_2(myzip)[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the local time zone, we can shift bid_timestamp_utc to local time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'UTC timestamp = {df_27.bid_timestamp_utc.iloc[0]}')\n",
    "print(f'local timestamp = {df_27.bid_timestamp_utc.iloc[0].tz_convert(\"America/New_York\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process is slow, so run it on each .csv file separately and combine the results at the end.\n",
    "\n",
    "With the local timestamp, we can extract quantities like hour, day and day_of_week.\n",
    "\n",
    "The functions reshape_files() and local_hour_creator() do the aforementioned preprocessing and save each column as a .gzip file. This step takes about 30-40 minutes on a laptop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization: maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the data in memory, we can make visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choropleths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "state_geo = 'us-states.json' # local copy of json file\n",
    "\n",
    "# load relevant data from disk\n",
    "data_dir = r'C:\\PythonBC\\RootData'\n",
    "df_clicks = lf.temp_load( os.path.join(data_dir, 'clicks.gzip')  )\n",
    "df_state = lf.temp_load( os.path.join(data_dir, 'state.gzip')  )\n",
    "df_installs = lf.temp_load( os.path.join(data_dir, 'installs.gzip')  )\n",
    "frames = [df_state, df_clicks, df_installs]\n",
    "df = pd.concat(frames, axis=1)\n",
    "\n",
    "# number of clicks per state\n",
    "dfstates = df.groupby('state').sum()['clicks'].to_frame()\n",
    "dfstates.reset_index(level=0, inplace=True)\n",
    "\n",
    "# number of bids per state\n",
    "dfstates2 = df.groupby('state').count()['clicks'].to_frame()\n",
    "dfstates2.reset_index(level=0, inplace=True)\n",
    "dfstates2.rename(index=str, columns={\"state\": \"state\", \"clicks\": \"bids\"})\n",
    "\n",
    "# number of installs per state\n",
    "dfstates3 = df.groupby('state').sum()['installs'].to_frame()\n",
    "dfstates3.reset_index(level=0, inplace=True)\n",
    "dfstates3.rename(index=str, columns={\"state\": \"state\", \"clicks\": \"installs\"})\n",
    "\n",
    "# build new dataframe\n",
    "bids = dfstates2.clicks.values\n",
    "clicks = dfstates.clicks.values\n",
    "installs = np.asarray(dfstates3.installs)\n",
    "state = dfstates.state.values\n",
    "clickrate = 100*np.divide(clicks, bids)\n",
    "installrate = 100*np.divide(installs, bids)\n",
    "frames = {\"state\": state, \"bids\": bids, \"clicks\": clicks, \"installs\": installs, \"clickrate\":clickrate, \"installrate\": installrate}\n",
    "df_rate = pd.DataFrame(data=frames)\n",
    "df_rate_nonzero = df_rate[df_rate.clickrate > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clickrate_m = folium.Map(location=[39.50, -98.35], zoom_start=4) # lower 48\n",
    "folium.Choropleth(\n",
    "    geo_data=state_geo,\n",
    "    name='choropleth',\n",
    "    data=df_rate_nonzero,\n",
    "    columns=['state', 'clickrate'],\n",
    "    key_on='feature.id',\n",
    "    fill_color='YlGn',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.2,\n",
    "    legend_name='click rate (%)'\n",
    ").add_to(clickrate_m)\n",
    "folium.LayerControl().add_to(clickrate_m)\n",
    "clickrate_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "installrate_m = folium.Map(location=[39.50, -98.35], zoom_start=4) # lower 48\n",
    "folium.Choropleth(\n",
    "    geo_data=state_geo,\n",
    "    name='choropleth',\n",
    "    data=df_rate_nonzero,\n",
    "    columns=['state', 'installrate'],\n",
    "    key_on='feature.id',\n",
    "    fill_color='YlGn',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.2,\n",
    "    legend_name='install rate (%)'\n",
    ").add_to(installrate_m)\n",
    "folium.LayerControl().add_to(installrate_m)\n",
    "installrate_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium.plugins import HeatMap\n",
    "\n",
    "zipc = lf.temp_load( os.path.join(data_dir, 'geo_zip.gzip'))\n",
    "click = lf.temp_load( os.path.join(data_dir, 'clicks.gzip'))\n",
    "df = pd.concat([zipc,click],axis=1)\n",
    "\n",
    "dftest = df.query('clicks == True')\n",
    "dftest = pd.DataFrame(dftest.geo_zip.value_counts())\n",
    "df = pd.DataFrame(df.geo_zip.value_counts())\n",
    "df['latitude'] = zc.zip_to_lat_2(df.index)\n",
    "df['longitude'] = zc.zip_to_lon_2(df.index)\n",
    "\n",
    "dfratio = dftest/df\n",
    "dfratio['latitude'] = zc.zip_to_lat_2(dfratio.index)\n",
    "dfratio['longitude'] = zc.zip_to_lon_2(dfratio.index)\n",
    "\n",
    "df=df.dropna()\n",
    "dfratio=dfratio.dropna()\n",
    "\n",
    "df = df[['latitude','longitude','geo_zip']]\n",
    "df_copy = df.copy()\n",
    "\n",
    "dfratio = dfratio[['latitude','longitude','geo_zip']]\n",
    "dfratio_copy = dfratio.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Heatmap of ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location=[38.5, -100],zoom_start=4)\n",
    "HeatMap(data=df_copy[['latitude', 'longitude', 'geo_zip']].groupby(['latitude', 'longitude']).\\\n",
    "        sum().reset_index().values.tolist(), radius=6, max_zoom=13).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap of (clicks/bids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mratio = folium.Map(location=[38.5, -100],zoom_start=4)\n",
    "HeatMap(data=dfratio_copy[['latitude', 'longitude', 'geo_zip']].groupby(['latitude', 'longitude']).\\\n",
    "        sum().reset_index().values.tolist(), radius=8, max_zoom=13).add_to(mratio)\n",
    "mratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a 'click' classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal was to create a model which will determine when an impression is likely to lead to a 'click' on the advertisement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is loaded into a dataframe and some columns are dropped. Masks are generated to split the data between the first three weeks and the last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for f in tqdm(glob.glob(os.path.join(data_directory, '*.gzip'))):\n",
    "    df = pd.concat([df,lf.temp_load(fname=f)], axis=1)\n",
    "\n",
    "df = df.drop(['app_bundle','bid_timestamp_utc', 'tz', 'spend', 'installs'], axis=1)\n",
    "df['inventory_interstitial'] = df['inventory_interstitial'].astype(int)\n",
    "df['rewarded'] = df['rewarded'].astype(int)\n",
    "df['clicks'] = df['clicks'].astype(int)\n",
    "\n",
    "start_date = pd.Timestamp('2019-04-23 00:00')\n",
    "end_date = pd.Timestamp('2019-04-30 00:00')\n",
    "mask_lastweek = (df['bid_timestamp_local'] > start_date) & (df['bid_timestamp_local'] <= end_date)\n",
    "\n",
    "start_date = pd.Timestamp('2019-04-01 00:00')\n",
    "end_date = pd.Timestamp('2019-04-23 00:00')\n",
    "mask_threewks = (df['bid_timestamp_local'] > start_date) & (df['bid_timestamp_local'] <= end_date)\n",
    "\n",
    "df = df.drop(['bid_timestamp_local'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of the data provided in this data set is categorical in nature. This data needs to be turned into numerical values for models to train on it.\n",
    "\n",
    "There are several methods to accomplish this:\n",
    "\n",
    "- One Hot Encoding\n",
    "- Hashing\n",
    "- Leave One Out\n",
    "- Ordinal\n",
    "- Binary\n",
    "- ... and many more\n",
    "\n",
    "The scikit-learn-contrib package [category_encoders](https://contrib.scikit-learn.org/categorical-encoding/) provides convenient transformers for many common methods.\n",
    "\n",
    "The encoding method used below is LeaveOneOut, which is tends to work well for high cardinality categorical data.\n",
    "\n",
    "From the [documentation](https://contrib.scikit-learn.org/categorical-encoding/targetencoder.html):\n",
    "\n",
    "`For the case of categorical target: features are replaced with a blend of posterior probability of the target given particular categorical value and the prior probability of the target over all the training data.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "y = df.clicks.values\n",
    "X = df.drop(['clicks'], axis=1)\n",
    "loo = ce.LeaveOneOutEncoder()\n",
    "X = loo.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling and splitting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data needs to also be scaled for the model to fit properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionaly, the data is split into the first three weeks and the last week.  The model will be trained and validated on the first three weeks, and then will be used to predict 'clicks' on the last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_threeweek = X[mask_threewks]\n",
    "y_threeweek = y[mask_threewks]\n",
    "X_lastweek = X[mask_lastweek]\n",
    "y_lastweek = y[mask_lastweek]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_threeweek, y_threeweek, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we implement is logistic regression with stochastic gradient descent (SGD).  This was done using SGDClassifier from scikit-learn.  Regularization was implemented by Elastic Net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "model = SGDClassifier(loss='log', penalty='elasticnet', alpha=0.001,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One might be tempted to train a model on the data set now that it has been encoded and scaled, but this would yield an accurate model that is useless.  This is clear when you look at the confusion matrx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred =  model.predict(X_test)\n",
    "acc_score = model.score(X_test, y_test)\n",
    "rec_score = recall_score(y_test, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "fig.suptitle(f'accuracy score: {acc_score:.3f, recall score: {rec_score:.3f}, fontsize=24)\n",
    "mm.plot_confusion_matrix(y_test, y_pred, ax=ax[0], normalize=True)\n",
    "mm.plot_confusion_matrix(y_test, y_pred, ax=ax[1], normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img\\Imbalanced-with-recall.png)\n",
    "\n",
    "Essentially, always choosing 'no click' yields a very accurate model since ~95% of impressions don't yield a click.  This is reflected in the recall score, which captures the ability of the model to find the positive samples ('clicks' in this case).\n",
    "\n",
    "![title](img\\precision-recall-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling to balance the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way to handle this is to not train on an unbalanced dataset.  There are several methods to accomplish this, but they fall into the following categories:\n",
    "\n",
    "Under-sampling:\n",
    "- Random under-sampling\n",
    "- NearMiss\n",
    "- AllKNN\n",
    "- ... and others\n",
    "\n",
    "Over-sampling:\n",
    "- Random over-sampling\n",
    "- SMOTE\n",
    "- SMOTENC\n",
    "- ADASYN\n",
    "\n",
    "Combined over- and under-sampling:\n",
    "- SMOTEENN\n",
    "- SMOTETomek\n",
    "\n",
    "We implement the two simplest ones, random over- and random under-sampling using the scikit-learn-contrib package [imbalanced-learn](https://imbalanced-learn.readthedocs.io/en/stable/index.html).\n",
    "\n",
    "![title](img\\under-vs-oversampling-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "sm = RandomOverSampler()\n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "model.fit(X_train_res, y_train_res)\n",
    "y_pred =  model.predict(X_test)\n",
    "acc_score = model.score(X_test, y_test)\n",
    "rec_score = recall_score(y_test, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "fig.suptitle(f'accuracy score: {acc_score:.3f}, recall score: {rec_score:.3f}', fontsize=24)\n",
    "mm.plot_confusion_matrix(y_test, y_pred, ax=ax[0], normalize=True)\n",
    "mm.plot_confusion_matrix(y_test, y_pred, ax=ax[1], normalize=False)\n",
    "\n",
    "\n",
    "sm = RandomUnderSampler()\n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "model.fit(X_train_res, y_train_res)\n",
    "y_pred =  model.predict(X_test)\n",
    "acc_score = model.score(X_test, y_test)\n",
    "rec_score = recall_score(y_test, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "fig.suptitle(f'accuracy score: {acc_score:.3f}, recall score: {rec_score:.3f}', fontsize=24)\n",
    "mm.plot_confusion_matrix(y_test, y_pred, ax=ax[0], normalize=True)\n",
    "mm.plot_confusion_matrix(y_test, y_pred, ax=ax[1], normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train on 70% of the data from the first three weeks and validate using the remaining 30%.  The resulting confusion matricies look much better! Clearly, the model is doing a much better job of capturing 'clicks' once it is trained on a balanced dataset. \n",
    "\n",
    "Over-sampled:\n",
    "![title](img\\RandomOverSampler_firstweek.png)\n",
    "\n",
    "Under-sampled:\n",
    "![title](img\\RandomUnderSampler_firstweek.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same model can now be used to predict 'clicks' for the last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred =  model.predict(X_lastweek)\n",
    "acc_score = model.score(X_lastweek, y_pred)\n",
    "rec_score = recall_score(y_lastweek, y_pred)\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "fig.suptitle(f'accuracy score: {acc_score:.3f}, recall score: {rec_score:.3f}', fontsize=24)\n",
    "mm.plot_confusion_matrix(y_lastweek, y_pred, ax=ax[0], normalize=True)\n",
    "mm.plot_confusion_matrix(y_lastweek, y_pred, ax=ax[1], normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over-sampled:\n",
    "![title](img\\RandomOverSampler_lastweek.png)\n",
    "\n",
    "Under-sampled:\n",
    "![title](img\\RandomUnderSampler_lastweek.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model seems to do reasonably well at predicting 'clicks', but there are several areas where it can be improved.\n",
    "- Dimensionality reduction should be explored using PCA or TruncatedSVD\n",
    "- Different encoding and scaling methods might be more appropriate for some of the data\n",
    "- More sophisticated methods for over- and under-sampling could yield better results, especially after dimension reduction\n",
    "- Different/more sophisticated models could be more appropriate for this data set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
